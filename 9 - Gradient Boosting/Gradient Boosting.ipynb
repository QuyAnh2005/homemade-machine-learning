{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d9b063b",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b7e7e2",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "### 1.1 Definition of Gradient Boosting\n",
    "Gradient Boosting is a machine learning algorithm that belongs to the ensemble learning category. It combines multiple weak models, usually decision trees, to create a single, strong prediction model. The algorithm works by iteratively adding new trees to the model and adjusting their weights based on the gradient of the loss function. The objective of the algorithm is to minimize the loss function, which measures the difference between the actual values and the predicted values. The final prediction model is a weighted sum of all the trees in the ensemble, where the weights are determined based on the optimization of the loss function. Gradient Boosting has proven to be effective for various real-world problems, such as classification and regression tasks, due to its ability to handle complex non-linear relationships and its high accuracy.\n",
    "\n",
    "### 1.2 Overview of ensemble learning\n",
    "Ensemble learning is a machine learning technique that involves combining multiple models to form a single, stronger prediction model. The idea behind ensemble learning is that a group of weak models can be combined to form a strong model that outperforms any individual model. Ensemble learning can be used for both classification and regression problems.\n",
    "\n",
    "There are several types of ensemble learning methods, including:\n",
    "\n",
    "- Bagging (Bootstrap Aggregating): involves training multiple models on different samples of the training data, with each sample being randomly drawn with replacement. The final prediction is obtained by averaging (or voting) the predictions of the individual models. (such as Random Forest algorithm that we discussed previous notebook)\n",
    "- Boosting: involves training multiple models in a sequential manner, where each new model aims to correct the mistakes made by the previous model. The final prediction is obtained by weighted sum of all the individual models. Gradient Boosting is a type of boosting algorithm.\n",
    "\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\"><img src='images/Boosting.png' alt=\"alternate text\" title=\"Boosting\" width=\"width\" height=\"height\" style=\"width:500px;height:250px;\" />  </div>\n",
    "<center> Image: Simulation for boosting technique </center>\n",
    "\n",
    "- Stacking: involves training multiple models on the same training data and using their predictions as inputs to train a final meta-model. The final prediction is obtained by using the predictions of the meta-model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3636b40d",
   "metadata": {},
   "source": [
    "## 2. Theoretical foundations of Gradient Boosting\n",
    "\n",
    "### 2.1 Introduction to decision trees\n",
    "Decision trees are a fundamental component of Gradient Boosting. They are simple yet powerful machine learning models used for both classification and regression tasks. The main idea behind decision trees is to split the data into smaller and smaller subsets, where each split is based on a single feature of the data. The end result is a tree-like structure that represents a series of decisions based on the features of the data.\n",
    "\n",
    "Each node in the tree represents a decision based on the value of a feature, and each branch of the tree represents the outcome of that decision. The final prediction is obtained by starting at the root node and following the branches of the tree based on the values of the features, until a leaf node is reached. The prediction is given by the value assigned to the leaf node.\n",
    "Decision trees are simple to understand, interpret, and visualize, making them a popular choice for many real-world problems. They are also robust to outliers, missing values, and noisy data, and can handle both categorical and numerical data.\n",
    "\n",
    "In mathematics, a tree can be formally expressed as\n",
    "\n",
    "$$T(x, \\Theta) = \\sum_{j=1}^{J} \\gamma_j I(x \\in R_j)$$\n",
    "\n",
    "with parameters $\\Theta = \\{R_j, \\gamma_j\\}_1^J$. $J$ is usually treated as a meta-parameter. The parameters are found by minimizing the empirical risk\n",
    "\n",
    "$$\\hat{\\Theta} = \\arg \\min_{\\Theta} \\sum_{j=1}^J \\sum_{x_i \\in R_j} L(y_i, \\gamma_j) = \\arg \\min_{\\Theta} \\sum_{j=1}^J L(y_i, T(x_i, \\Theta))$$\n",
    "\n",
    "where a tree partition the space of all joint predictor variable values into disjoint regions $R_j, j=1,2,...,J$, as represented by the terminal nodes of the tree. A constant $\\gamma_j$ is assigned to each such region and the predictive rule is\n",
    "\n",
    "$$x \\in R_j \\Rightarrow f(x) = \\gamma_j$$\n",
    "\n",
    "- $N$ is the total number of observations\n",
    "- $L$ is the loss function\n",
    "- $x_i$ is $i-th$ observation\n",
    "- $y_i$ is label of $x_i$ in datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201eb9d",
   "metadata": {},
   "source": [
    "### 2.2 Mathematical formulation of Gradient Boosting\n",
    "\n",
    "In the context of Gradient Boosting, decision trees are used as weak models, which are combined to form a single, strong model. The algorithm works by iteratively adding new trees to the model and adjusting their weights based on the **gradient of the loss function**. The objective of the algorithm is to minimize the loss function, which measures the difference between the actual values and the predicted values.\n",
    "\n",
    "The boosted tree model is a sum of such trees\n",
    "\n",
    "$$f_M(x) = \\sum_{m=1}^{M} T(x, \\Theta_m)$$\n",
    "\n",
    "At each step in the forward stagewise procedure one must solve \n",
    "\n",
    "$$\\hat{\\Theta}_m = \\arg \\min_{\\Theta_m} \\sum_{i=1}^{N} L(x_i, f_{m-1}(x_i) + T(x_i, \\Theta_m))$$\n",
    "\n",
    "for the region set and constants $\\Theta_m = \\{R_{jm}, \\gamma_{jm}\\}$ of the next tree, given the current model $f_{m-1}(x)$ and \n",
    "\n",
    "$$\\hat{\\gamma}_{jm} = \\arg \\min_{\\gamma_{jm}} \\sum_{x \\in R_{jm}} L(y_i, f_{m-1}(x_i) + \\gamma_{jm})$$\n",
    "\n",
    "#### Numerical Optimization via Gradient Descent\n",
    "The loss in using $f(x)$ to predict $y$ on the training data is\n",
    "\n",
    "$$L(f) = \\sum_{i=1}^{N} L(y_i, f(x_i))$$\n",
    "\n",
    "or you need to find\n",
    "\n",
    "$$\\hat{f} = \\arg \\min_{f} L(f)$$\n",
    "\n",
    "where the \"paramters\" $f \\in \\mathbb{R}^N$. Numerical optimization procedures solve the above as a sum of component vectors\n",
    "\n",
    "$$f_m = \\sum_{m=0}^{M} h_m$$\n",
    "\n",
    "where $\\_m \\in \\mathbb{R}^N, f_0 = h_0$ is an initial guess, and each successive $f_m$ is induced based on the current parameter vector $f_{m-1}$, which is the sum of the previous induced update. Steepest descent chooses $h_m = -\\lambda_m g_m$ where $\\lambda_m$ is a scalar and $g_m \\in \\mathbb{R}^N$ is the gradient of $L(f)$ evaluated at $f = f_{m-1}$\n",
    "\n",
    "$$g_m = \\left[ \\frac{\\partial L(y, f(x))}{\\partial f(x)} \\right]_{f(x) = f_{m-1}(x)}$$\n",
    "\n",
    "The *step length (learning rate)* $\\lambda_m$ is the solution to \n",
    "\n",
    "$$\\lambda_m = \\arg \\min_{\\lambda} L(f_{m-1} - \\lambda g_m)$$\n",
    "\n",
    "The current solution is then updated \n",
    "\n",
    "$$f_m = f_{m-1} - \\lambda_m g_m$$\n",
    "\n",
    "**Note:** In practice, we often fix $\\lambda_m$ by specific scaler (e.g $0.3, 0.5, 1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29580e",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "From **Theoretical foundations of Gradient Boosting**, we have pseudo-code for regression problem.\n",
    "\n",
    "\n",
    "1. Initialize $f_0 (x) = \\arg \\min_{\\gamma} \\sum_{i=1}^{N} L(y_i, \\gamma)$\n",
    "\n",
    "2. For $m=1$ to $M$:\n",
    "    \n",
    "    - (a) For $i=1,2,..., N$ compute \n",
    "    $$r_{im} = - \\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} \\right]_{f = f_{m-1}}$$\n",
    "    \n",
    "    - (b) Fit a regression tree to targets $r_{im}$ giving terminal regions $R_{jm}, j = 1, 2, ...., J_m$\n",
    "    \n",
    "    - (c) For $j = 1, 2, ...., J_m$ compute\n",
    "    $$\\gamma_{jm} = \\arg \\min_{\\gamma} \\sum_{x_i \\in R_{jm}} L(y_i, f_{m-1}(x_i) + \\gamma)$$\n",
    "    \n",
    "    - (d) Update $f_m(x) = f_{m-1}(x) + \\lambda_m \\sum_{j=1}^{J_m} \\gamma_{jm}I(x \\in R_{jm})$\n",
    "    \n",
    "3. Output $\\hat{f}(x) = f_M (x)$\n",
    "\n",
    "In this implementation, we will use square error loss\n",
    "\n",
    "$$L(y_i, f(x_i)) = (y_i - f(x_i))^2$$\n",
    "\n",
    "$$\\Rightarrow - \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} = y_i - f(x_i)$$\n",
    "\n",
    "and the solution for (c) is the **mean** of residuals of (a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "492e0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "class GradientBoosting():\n",
    "    \"\"\"Implement Gradent Boosting algorithm for regression problem.\n",
    "    \n",
    "    Paramters:\n",
    "        n_estimators (int): number of base models, default 100\n",
    "        lambd (float): learning rate, default 1.0\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=100, lambd=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.lambd = lambd\n",
    "        self.trees = []\n",
    "        \n",
    "    def _initialize(self, y):\n",
    "        \"\"\"Initialize f0 from all labels y.\"\"\"\n",
    "        n_observations = len(y)\n",
    "        self.init_gamma = np.mean(y)\n",
    "        f0 = np.full((n_observations, ), self.init_gamma)\n",
    "        return f0\n",
    "        \n",
    "    def _compute_gamma(self, residuals, leaf_ids):\n",
    "        \"\"\"Compute the optimal gamma.\n",
    "        \n",
    "        Args:\n",
    "            residuals: error \n",
    "            leaf_ids: indices of leaves corresponding observations\n",
    "            \n",
    "        Return:\n",
    "            gamma: the optimal value for gamma\n",
    "        \"\"\"\n",
    "        leaves = np.unique(leaf_ids)\n",
    "        gamma = np.zeros(residuals.shape)\n",
    "        for leaf in leaves:\n",
    "            ids = np.where(leaf_ids == leaf)[0]\n",
    "            gamma[ids] = np.mean(residuals[ids])\n",
    "            \n",
    "        return gamma\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Training model from given datasets.\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.array): Array of feature\n",
    "            y (numpy.array): Array of output \n",
    "        \"\"\"\n",
    "        # Step 1 - Initialize f0\n",
    "        self.f0 = self._initialize(y)\n",
    "        # Assign current f by f0\n",
    "        f_current = self.f0\n",
    "        \n",
    "        # Step 2 - Loop M times\n",
    "        for i in range(self.n_estimators):\n",
    "            # Step 2 (a) - Calculate residuals\n",
    "            residuals = y - f_current\n",
    "            \n",
    "            # Step 2 (b) - Fit a regression tree to residuals\n",
    "            clf = DecisionTreeRegressor()\n",
    "            clf.fit(X, residuals)\n",
    "            leaf_ids = clf.apply(X)    # Get terminal region R_j\n",
    "            \n",
    "            # Step 2 (c) - Compute gamma_j corresponding R_j\n",
    "            gamma = self._compute_gamma(residuals, leaf_ids)\n",
    "            \n",
    "            # Step 2 (d) - Update\n",
    "            f_current += self.lambd * gamma\n",
    "            \n",
    "            # Append base model to predict in the future\n",
    "            self.trees.append(clf)\n",
    "            \n",
    "            \n",
    "    def predict(self, X_new):\n",
    "        \"\"\"Predict from given input.\n",
    "        \n",
    "        Args:\n",
    "            X_new (numpy.array): given input that need to be predicted\n",
    "            \n",
    "        Return:\n",
    "            preds: the prediction for given X_new from Gradient Boosting algorithm\n",
    "        \"\"\"\n",
    "        # Assign preds by f0 as in the training\n",
    "        size = X_new.shape[0]\n",
    "        preds = np.full((size,), self.init_gamma)\n",
    "        \n",
    "        # Update the preds from model 1 to model M\n",
    "        for tree in self.trees:\n",
    "            residuals = tree.predict(X_new)\n",
    "            leaf_ids = tree.apply(X_new)\n",
    "            gamma = self._compute_gamma(residuals, leaf_ids)\n",
    "            preds += self.lambd * gamma\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f8046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Fit model\n",
    "n_estimators = 100\n",
    "lambd = 0.1\n",
    "gb = GradientBoosting(n_estimators, lambd)\n",
    "gb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609e834a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.183580705289566e-06"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # Validate model\n",
    "preds = gb.predict(X)\n",
    "mean_squared_error(preds, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4fe9a",
   "metadata": {},
   "source": [
    "## 4. Pros and cons\n",
    "**Advantages:**\n",
    "- Flexibility: Gradient Boosting can be applied to a wide range of machine learning problems, including regression, classification, and ranking problems.\n",
    "- Ability to handle non-linear relationships: Gradient Boosting can capture complex non-linear relationships between the features and the target variable by combining the predictions of multiple decision trees.\n",
    "- High accuracy: Gradient Boosting is known for its high accuracy and has been used to win many machine learning competitions.\n",
    "- Feature selection: Gradient Boosting can perform feature selection implicitly by assigning a small weight to the features that are not important to the prediction.\n",
    "- Reduction of bias: Bias is the presence of uncertainty or inaccuracy in machine learning results. Boosting algorithms combine multiple weak learners in a sequential method, which iteratively improves observations. This approach helps to reduce high bias that is common in machine learning models.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Vulnerability to outlier data: Boosting models are vulnerable to outliers or data values that are different from the rest of the dataset. Because each model attempts to correct the faults of its predecessor, outliers can skew results significantly.\n",
    "- Computational cost: Gradient Boosting is computationally expensive as it requires training a large number of decision trees.\n",
    "- Overfitting: Gradient Boosting is prone to overfitting, especially when the number of trees is too large or the tree complexity is too high.\n",
    "- Difficult to interpret: Gradient Boosting models are complex and can be difficult to interpret. The relationships between the features and the target variable are encoded in the structure of the decision trees, which can be difficult to understand.\n",
    "- Slower prediction time: Gradient Boosting models have a slower prediction time compared to linear models, as they require evaluating the predictions of multiple decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2250545a",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting)\n",
    "- [Gradient Tree Boosting](https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12.pdf#page=373)\n",
    "- [Introduction to Boosted Trees](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)\n",
    "- [What Is Boosting?](https://aws.amazon.com/what-is/boosting/)\n",
    "- [How XGBoost Works](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-HowItWorks.html)\n",
    "- [XGBoost](https://en.wikipedia.org/wiki/XGBoost)\n",
    "- [An End-to-End Guide to Understand the Math behind XGBoost](https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/)\n",
    "- [A Gentle Introduction to XGBoost for Applied Machine Learning](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
