{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf67af9",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm\n",
    "\n",
    "## Overview\n",
    "- [1. What is Decision Tree](#1)\n",
    "- [2. Important Terminology related to Decision Trees](#2)\n",
    "- [3. How do Decision Trees work?](#3)\n",
    "- [4. ID3 Algorithm](#4)\n",
    "    - [4.1 Entropy](#4.1)\n",
    "    - [4.2 Infomation Gain](#4.2)\n",
    "    - [4.2 Split a node into branches](#4.3)\n",
    "- [5. Model from scratch and predict](#5)\n",
    "- [6. Decision Tree in sklearn and some notes](#6)\n",
    "    - [6.1 CART cost function for classification](#6.1)\n",
    "    - [6.2 CART cost function for regression](#6.2)\n",
    "- [7. Pros and cons](#7)\n",
    "- [8. References](#8)\n",
    "\n",
    "\n",
    "### Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d471fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b405904",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cbddb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlook</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind</th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sunny</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>weak</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunny</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>strong</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>overcast</td>\n",
       "      <td>hot</td>\n",
       "      <td>high</td>\n",
       "      <td>weak</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rainy</td>\n",
       "      <td>mild</td>\n",
       "      <td>high</td>\n",
       "      <td>weak</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rainy</td>\n",
       "      <td>cool</td>\n",
       "      <td>normal</td>\n",
       "      <td>weak</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    outlook temperature humidity    wind play\n",
       "0     sunny         hot     high    weak   no\n",
       "1     sunny         hot     high  strong   no\n",
       "2  overcast         hot     high    weak  yes\n",
       "3     rainy        mild     high    weak  yes\n",
       "4     rainy        cool   normal    weak  yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pd.read_csv('weather.csv')\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c43df55",
   "metadata": {},
   "source": [
    "<a name='1' ></a>\n",
    "## 1. What is Decision Tree?\n",
    "Decision Tree algorithm belongs to the family of supervised learning algorithms. Unlike other supervised learning algorithms, the decision tree algorithm can be used for solving **regression and classification** problems too.\n",
    "\n",
    "The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n",
    "\n",
    "In Decision Trees, for predicting a class label for a record we start from the root of the tree. We compare the values of the root attribute with the record’s attribute. On the basis of comparison, we follow the branch corresponding to that value and jump to the next node.\n",
    "\n",
    "**Decision tree types:**\n",
    "- **Classification tree** analysis is when the predicted outcome is the class (discrete) to which the data belongs\n",
    "- **Regression tree** analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patient's length of stay in a hospital)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd9001",
   "metadata": {},
   "source": [
    "<a name='2' ></a>\n",
    "## 2. Important Terminology related to Decision Trees\n",
    "- **Root Node**: It represents the entire population or sample and this further gets divided into two or more homogeneous sets\n",
    "- **Splitting**: It is a process of dividing a node into two or more sub-nodes\n",
    "- **Decision Node**: When a sub-node splits into further sub-nodes, then it is called the decision node\n",
    "- **Leaf/Terminal Node**: Nodes do not split is called Leaf or Terminal node\n",
    "- **Pruning**: When we remove sub-nodes of a decision node, this process is called pruning. You can say the opposite process of splitting\n",
    "- **Branch/Sub-Tree**: A subsection of the entire tree is called branch or sub-tree\n",
    "- **Parent and Child Node**: A node, which is divided into sub-nodes is called a parent node of sub-nodes whereas sub-nodes are the child of a parent node\n",
    "\n",
    "<div style=\"width:image width px; font-size:80%; text-align:center;\"><img src='images/structure-tree.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:700px;height:400px;\" />  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8daf3511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        ids: list = None, \n",
    "        children: list = [], \n",
    "        entropy: float = 0.0, \n",
    "        depth: int = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Represent a node in tree\n",
    "        \n",
    "        Args:\n",
    "            ids: (int) index of data in this node\n",
    "            children: (list) list of its child nodes\n",
    "            entropy: (float)\n",
    "            depth: (int) distance from this node to root node\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ids = ids  \n",
    "        self.children = children \n",
    "        self.entropy = entropy\n",
    "        self.depth = depth      \n",
    "        self.split_attribute = None     # which attribute is chosen, it non-leaf\n",
    "        self.order = None     # order of values of split_attribute in children\n",
    "        self.label = None     # label of node if it is a leaf\n",
    "\n",
    "    def set_properties(self, split_attribute, order):\n",
    "        self.split_attribute = split_attribute\n",
    "        self.order = order\n",
    "\n",
    "    def set_label(self, label):\n",
    "        self.label = label\n",
    "        \n",
    "    def __str__(self):\n",
    "        information = (\n",
    "            f'Index of node: {self.ids}\\n'\n",
    "            f'Children: {self.children}\\n'\n",
    "            f'Entropy: {self.entropy}\\n'\n",
    "            f'Depth: {self.depth}'\n",
    "        )\n",
    "        return information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44355425",
   "metadata": {},
   "source": [
    "<a name='3' ></a>\n",
    "## 3. How do Decision Trees work?\n",
    "The decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria are different for classification and regression trees.\n",
    "\n",
    "Decision trees use multiple algorithms to decide to split a node into two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that the purity of the node increases with respect to the target variable. The decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.\n",
    "\n",
    "The algorithm selection is also based on the type of target variables. Let us look at some algorithms used in Decision Trees:\n",
    "- [ID3](https://en.wikipedia.org/wiki/ID3_algorithm) (Iterative Dichotomiser 3)\n",
    "- [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm) (successor of ID3)\n",
    "- [CART](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29) (Classification And Regression Tree)\n",
    "- [Chi-square automatic interaction detection (CHAID)](https://en.wikipedia.org/wiki/Chi-square_automatic_interaction_detection). Performs multi-level splits when computing classification trees\n",
    "- [MARS](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines): extends decision trees to handle numerical data better\n",
    "\n",
    "ID3 and CART were invented independently at around the same time (between 1970 and 1980), yet follow a similar approach for learning a decision tree from training tuples. **In this lecture, We firstly concentrate on ID3 Algorithm for DT (Decision Tree).**\n",
    "\n",
    "<a name='4' ></a>\n",
    "## 4. ID3 Algorithm \n",
    "The ID3 algorithm begins with the original set $S$ as the root node. On each iteration of the algorithm, it iterates through every unused attribute of the set $S$ and calculates the entropy ${H}(S)$ or the information gain $IG(S)$ of that attribute. It then selects the attribute which has the smallest entropy (or largest information gain) value. The set $S$ is then split or partitioned by the selected attribute to produce subsets of the data. (For example, a node can be split into child nodes based upon the subsets of the population whose ages are less than 50, between 50 and 100, and greater than 100.) The algorithm continues to recurse on each subset, considering only attributes never selected before.\n",
    "\n",
    "**Steps in ID3:**\n",
    "- Calculate the entropy of every attribute $a$ of the data set $S$.\n",
    "- Partition (\"split\") the set $S$ into subsets using the attribute for which the resulting entropy after splitting is minimized; or, equivalently, information gain is maximum.\n",
    "- Make a decision tree node containing that attribute.\n",
    "- Recurse on subsets using the remaining attributes.\n",
    "\n",
    "**Recursion on a subset may stop in one of these cases:**\n",
    "\n",
    "- every element in the subset belongs to the same class; in which case the node is turned into a leaf node and labelled with the class of the examples.\n",
    "- there are no more attributes to be selected, but the examples still do not belong to the same class. In this case, the node is made a leaf node and labelled with the most common class of the examples in the subset.\n",
    "- there are no examples in the subset, which happens when no example in the parent set was found to match a specific value of the selected attribute. An example could be the absence of a person among the population with age over 100 years. Then a leaf node is created and labelled with the most common class of the examples in the parent node's set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ab229",
   "metadata": {},
   "source": [
    "<a name='4.1' ></a>\n",
    "### 4.1 Entropy\n",
    "\n",
    "Entropy is a measure of the randomness in the information being processed. The higher the entropy, the harder it is to draw any conclusions from that information. \n",
    "\n",
    "In other word, Entropy $H(S)$ is a measure of the amount of uncertainty in the (data) set $S$ (i.e. entropy characterizes the (data) set $S$)\n",
    "\n",
    "$$H(S) = \\sum_{x \\in X} -p(x) \\log_2 p(x)$$\n",
    "where, \n",
    "- $S$: The current dataset for which entropy is being calculated\n",
    "- $X$: The set of classes in $S$\n",
    "- $p(x)$: The proportion of the number of elements in class $x$ to the number of elements in set $S$\n",
    "\n",
    "When $H(S)=0$, the set $S$ is perfectly classified (i.e. all elements in $S$ are of the same class). In ID3, entropy is calculated for each remaining attribute. The attribute with the smallest entropy is used to split the set $S$ on this iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82eb472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: ['no' 'no' 'yes' 'yes' 'yes' 'no' 'yes' 'no' 'yes' 'yes' 'yes' 'yes' 'yes'\n",
      " 'no'] \n",
      "Entropy: 0.6517565611726531\n"
     ]
    }
   ],
   "source": [
    "def entropy(target):\n",
    "    \"\"\"\n",
    "    Calculate entropy of the label set S\n",
    "    \n",
    "    Args:\n",
    "        target: (ndarray) labels in set S\n",
    "    Return:\n",
    "        en: (float) entropy value of input 'target'\n",
    "    \"\"\"\n",
    "    _, freqs = np.unique(target, return_counts=True)\n",
    "    probs = freqs / float(freqs.sum())\n",
    "    en = - np.sum(probs * np.log(probs))\n",
    "    return en\n",
    "    \n",
    "# Test function\n",
    "target = weather['play']\n",
    "en = entropy(target)\n",
    "print(f'Target: {target.values} \\nEntropy: {en}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0575010",
   "metadata": {},
   "source": [
    "<a name='4.2' ></a>\n",
    "### 4.2 Information Gain\n",
    "Information gain $IG(A)$ is the measure of the difference in entropy from before to after the set $S$ is split on an attribute $A$. In other words, how much uncertainty in $S$ was reduced after splitting set $S$ on attribute $A$.\n",
    "\n",
    "$$IG(S, A) = H(S) - \\sum_{t \\in T} p(t) H(t) = H(S) - H(S|A)$$\n",
    "where,\n",
    "- $H(S)$: Entropy of $S$\n",
    "- $T$: The subsets created from splitting set $S$ by attribute $A$ such that $S = \\cup_{t \\in T} t$\n",
    "- $p(t)$: The proportion of the number of elements in $t$ to the number of elements in set $S$\n",
    "- $H(t)$: Entropy of subset $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fbeafe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(node, T, target):\n",
    "    \"\"\"\n",
    "    Calculate information gain of a tree after spliting.\n",
    "    \n",
    "    Args:\n",
    "        node: (Node) (parent) node, node need to be splitted\n",
    "        T: (list) contain all indexs of each children\n",
    "        target: (ndarray) labels of set S\n",
    "    Return:\n",
    "        ig: information gain of node T after being splitted by attribute A\n",
    "    \"\"\"\n",
    "    n = len(node.ids)\n",
    "    H_SA = 0\n",
    "    for ids in T:\n",
    "        p = len(ids) / n\n",
    "        H_SA += p * entropy(target[ids])\n",
    "    ig = node.entropy - H_SA\n",
    "    \n",
    "    return ig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c46290",
   "metadata": {},
   "source": [
    "<a name='4.3' ></a>\n",
    "### 4.3 Split a node into branches\n",
    "In ID3, information gain can be calculated (instead of entropy) for each remaining attribute. The attribute with the **largest** information gain is used to split the set $S$ on this iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02c51052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(node, data, target):\n",
    "    \"\"\"\n",
    "    Function for making best split\n",
    "    \n",
    "    Args:\n",
    "        node: (Node) a node of tree\n",
    "        data: (DataFrame) original data from input\n",
    "        target: (ndarray) labels corresponds to each row in data\n",
    "    Return:\n",
    "        child_nodes: (list) contain all childrens of parent node\n",
    "    \"\"\"\n",
    "    ids = node.ids \n",
    "    best_gain = 0\n",
    "    min_gain = 1e-4\n",
    "    min_samples_split = 2\n",
    "    best_splits = []\n",
    "    best_attribute = None\n",
    "    order = None\n",
    "    sub_data = data.iloc[ids, :]\n",
    "    \n",
    "    for i, att in enumerate(attributes):\n",
    "        values = data.iloc[ids, i].unique().tolist()\n",
    "        if len(values) == 1: continue     # entropy = 0\n",
    "        splits = []\n",
    "        for val in values: \n",
    "            sub_ids = sub_data.index[sub_data[att] == val].tolist()\n",
    "            splits.append(sub_ids)\n",
    "        # don't split if a node has too small number of points\n",
    "        if min(map(len, splits)) < min_samples_split: continue\n",
    "        # information gain\n",
    "        gain = information_gain(node, splits, target)\n",
    "        if gain < min_gain: continue     # stop if small gain \n",
    "        if gain > best_gain:\n",
    "            best_gain = gain \n",
    "            best_splits = splits\n",
    "            best_attribute = att\n",
    "            order = values\n",
    "    \n",
    "    node.set_properties(best_attribute, order)\n",
    "    child_nodes = [Node(ids = split, entropy = entropy(target[split]), depth = node.depth+1) \n",
    "                   for split in best_splits]\n",
    "    \n",
    "    return child_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b47faff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ROOT------------------------------\n",
      "Index of node: range(0, 14)\n",
      "Children: [<__main__.Node object at 0x7f0325c24d00>, <__main__.Node object at 0x7f0325c353d0>, <__main__.Node object at 0x7f0325c35520>]\n",
      "Entropy: 0.6517565611726531\n",
      "Depth: 0\n",
      "------------------------------Children------------------------------\n",
      "--------------------------------------------------\n",
      "Index of node: [0, 1, 7, 8, 10]\n",
      "Children: []\n",
      "Entropy: 0.6730116670092565\n",
      "Depth: 1\n",
      "--------------------------------------------------\n",
      "Index of node: [2, 6, 11, 12]\n",
      "Children: []\n",
      "Entropy: -0.0\n",
      "Depth: 1\n",
      "--------------------------------------------------\n",
      "Index of node: [3, 4, 5, 9, 13]\n",
      "Children: []\n",
      "Entropy: 0.6730116670092565\n",
      "Depth: 1\n"
     ]
    }
   ],
   "source": [
    "# Create root node\n",
    "data = weather.iloc[:, :-1]\n",
    "target = weather.iloc[:, -1]\n",
    "attributes = list(data)\n",
    "ids = range(len(data))\n",
    "root = Node(ids=ids, entropy=entropy(target), depth=0)\n",
    "\n",
    "# Test function\n",
    "root.children = split(root, data, target)\n",
    "\n",
    "# Print tree\n",
    "print('-' * 30 + 'ROOT' + '-' * 30)\n",
    "print(root)\n",
    "# Print all children of root\n",
    "print('-' * 30 + 'Children' + '-' * 30)\n",
    "for child in root.children:\n",
    "    print('-' * 50)\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d9b853",
   "metadata": {},
   "source": [
    "<a name='5' ></a>\n",
    "## 5. Model from scratch and predict\n",
    "As a result, we can calculate entropy and information gain to generate a decision tree from the root node. And here is the Decision Tree (ID3 algorithm) pseudocode.\n",
    "\n",
    "\n",
    "```\n",
    "ID3 (Examples, Attributes, Target_Attribute)\n",
    "    Create a root node for the tree\n",
    "    If all examples are positive, Return the single-node tree Root, with label = +.\n",
    "    If all examples are negative, Return the single-node tree Root, with label = -.\n",
    "    If number of predicting attributes is empty, then Return the single node tree Root,\n",
    "    with label = most common value of the target attribute in the examples.\n",
    "    Otherwise Begin\n",
    "        A ← The Attribute that best classifies examples.\n",
    "        Decision Tree attribute for Root = A.\n",
    "        For each possible value, vi, of A,\n",
    "            Add a new tree branch below Root, corresponding to the test A = vi.\n",
    "            Let Examples(vi) be the subset of examples that have the value vi for A\n",
    "            If Examples(vi) is empty\n",
    "                Then below this new branch add a leaf node \n",
    "                with label = most common target value in the examples\n",
    "            Else below this new branch add the subtree ID3 (Examples(vi), Attributes – {A}, Target_Attribute)\n",
    "    End\n",
    "    Return Root\n",
    "```\n",
    "\n",
    "Don't worry, the ID3 algorithm was rebuilt from the above pseudocode. If you're curious, you can find more information in the file `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73b2bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DecisionTreeID3\n",
    "\n",
    "X = data\n",
    "y = target\n",
    "tree = DecisionTreeID3(max_depth=3, min_samples_split=2)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Test\n",
    "y_pred = tree.predict(X)\n",
    "assert (y_pred == y.values).all(), 'Wrong model!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c985d",
   "metadata": {},
   "source": [
    "<a name='6' ></a>\n",
    "## 6. Decision Tree in `sklearn` and some notes \n",
    "\n",
    "*Scikit-Learn uses the CART algorithm, which produces only **binary trees**: nonleaf nodes always have two children (i.e., questions only have yes/no answers). However, other algorithms such as ID3 can produce Decision Trees with nodes that have more than two children.*\n",
    "\n",
    "`Scikit-Learn` uses the *Classification And Regression Tree (CART)* algorithm to train Decision Trees (also called “growing” trees). The idea is really quite simple: the algorithm first splits the training set in two subsets using a single feature $k$ and a threshold $t_k$. How does it choose $k$ and $t_k$ ? It searches for the pair $(k, t_k)$ that produces the purest subsets (weighted by their size). \n",
    "\n",
    "<a name='6.1' ></a>\n",
    "### 6.1 CART cost function for classification\n",
    "The cost function that the algorithm tries to minimize is given by \n",
    "\n",
    "$$J(k, t_k) = \\frac{m_{\\text{left}}}{m} G_{\\text{left}} + \\frac{m_{\\text{right}}}{m} G_{\\text{right}}$$\n",
    "\n",
    "where\n",
    "\\begin{cases}\n",
    "    G_{\\text{left/right}} \\text{ measures the impurity of the left/right subset} \\\\\n",
    "    m_{\\text{left/right}} \\text{ is the number of instances in the left/right subset}\n",
    "\\end{cases}\n",
    "\n",
    "and the impurity of subset maybe `gini` (default in `sklearn`) or `entropy`, you can select the entropy impurity measure instead by setting the `criterion` hyperparameter to `entropy`\n",
    "\n",
    "*Gini impurity is defined by:*\n",
    "$$G_i = 1 - \\sum_{k=1}^{n} p_{i, k}^2$$\n",
    "\n",
    "where $p_{i, k}$ is the ratio of class $k$ instances among the training instances in the $i^{th}$ node.\n",
    "\n",
    "**Note: Gini Impurity or Entropy?**\n",
    "\n",
    "`The truth is, most of the time it does not make a big difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees.`\n",
    "\n",
    "<a name='6.2' ></a>\n",
    "### 6.2 CART cost function for regression\n",
    "The CART algorithm works mostly the same way as earlier, except that instead of trying to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that minimizes the $MSE$\n",
    "\n",
    "$$J(k, t_k) = \\frac{m_{\\text{left}}}{m} MSE_{\\text{left}} + \\frac{m_{\\text{right}}}{m} MSE_{\\text{right}}$$\n",
    "\n",
    "where\n",
    "\\begin{cases}\n",
    "    MSE_{\\text{node}} = \\sum_{i \\in \\text{node}} (\\hat{y}_{\\text{node}} - y^{(i)})^2 \\text{ measures the impurity of the left/right subset} \\\\\n",
    "    \\hat{y}_{\\text{node}} = \\frac{1}{m_{\\text{node}}} \\sum_{i \\in \\text{node}} y^{(i)} \\\\\n",
    "    m_{\\text{left/right}} \\text{ is the number of instances in the left/right subset}\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9af8c",
   "metadata": {},
   "source": [
    "<a name='7' ></a>\n",
    "## 7. Pros and cons\n",
    "Although implemented using any algorithm, Decision Tree has the following advantages and disadvantages.\n",
    "\n",
    "**Some advantages of decision trees are:**\n",
    "- Simple to understand and to interpret. Trees can be visualized.\n",
    "- Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "- Able to handle both numerical and categorical data. However, the scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialized in analyzing datasets that have only one type of variable.\n",
    "- Able to handle multi-output problems.\n",
    "- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "- Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "**The disadvantages of decision trees include:**\n",
    "- Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "- Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations. Therefore, they are not good at extrapolation.\n",
    "- The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da6707",
   "metadata": {},
   "source": [
    "<a name='8' ></a>\n",
    "## 8. References\n",
    "- Nagesh Singh Chauhan, [Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)\n",
    "- [Decision tree learning](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "- [ID3 algorithm](https://en.wikipedia.org/wiki/ID3_algorithm)\n",
    "- sklearn, [Decision Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- [Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow (Page 177-189)](https://www.knowledgeisle.com/wp-content/uploads/2019/12/2-Aur%C3%A9lien-G%C3%A9ron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build-Intelligent-Systems-O%E2%80%99Reilly-Media-2019.pdf)\n",
    "- Blog Machine Learning Cơ bản, [Iterative Dichotomiser 3](https://machinelearningcoban.com/2018/01/14/id3/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
